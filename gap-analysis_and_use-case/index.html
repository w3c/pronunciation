<!DOCTYPE html>
<html lang="en"
      xml:lang="en"
      xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Pronunciation Gap Analysis</title>
  <meta charset="utf-8" />
  <script class="remove"
        src="https://www.w3.org/Tools/respec/respec-w3c-common"></script>
  <script class="remove"
        src="respec-config.js"></script>
</head>
<body>
  <section id="abstract">
    <p>The objective of the Pronunciation Task Force is to develop normative specifications and best practices guidance collaborating with other W3C groups as appropriate, to provide for proper pronunciation in HTML content when using text to speech (TTS) synthesis. This document presents the results of the Pronunciation Task Force work on an HTML standard. It includes an introduction with a historical perspective, an enumeration of the core requirements, a listing of approach use cases, and finally a gap analysis. Gaps are defined when a requirement does not have a corresponding use case approach by which it can be authored in HTML.</p>
  </section>
  <section id="sotd"></section>
  <section class="informative" id="introduction">
    <h1>Introduction</h1>
    <p>Accurate, consistent pronunciation and presentation of content spoken by text to speech synthesis (TTS) is an essential requirement in education, communication, entertainment, and other domains. 
          From helping to teach spelling and pronunciation in different languages, 
          TTS has become a vital technology for providing access to digital content on the web and through mobile devices. 
          Organizations such as educational publishers and assessment vendors are looking for a standards-based solution to 
          enable authoring of spoken presentation guidance in HTML which can then be consumed by assistive technologies and other 
          applications that utilize text to speech synthesis (TTS) for rendering of content. 
          Historically, efforts at standardization (e.g. SSML or CSS Speech) have not led to broad adoption of any standard by user agents, authors or assistive technologies; 
          what has arisen are a variety of non-interoperable approaches that meet specific needs for some applications.</p>
    <p> The W3C has developed two standards pertaining to the presentation of speech synthesis which have reached recommendation status, <a href="https://www.w3.org/TR/speech-synthesis11/">Speech Synthesis Markup Language</a> (SSML) and the <a href=
    "https://www.w3.org/TR/pronunciation-lexicon/">Pronunciation Lexicon Specification</a> (PLS). Both standards are directly consumed by a speech synthesis engine supporting those standards. While a PLS file reference may be referenced in a HTML
    page using <code>link rel</code>, there is no known uptake of PLS using this method by assistive technologies. While there are technically methods to allow authors to inline SSML within HTML (using namespaces), such an approach has not been
    adopted, and anecdotal comments from browser and assistive technology vendors have suggested this is not a viable approach.</p>
    <p>The <a href="https://www.w3.org/TR/css3-speech/">CSS Speech Module</a> is a retired W3C Working Group Note that describes a mechanism by which content authors may apply a variety of speech styling and presentation properties to HTML. This approach
    has a variety of advantages but does not implement the full set of features required for pronunciation. <a href="https://www.w3.org/TR/css3-speech/#pronunciation">Section 16</a> of the Note specifically references the issue of pronunciation:</p>
    <blockquote>
      CSS does not specify how to define the pronunciation (expressed using a well-defined phonetic alphabet) of a particular piece of text within the markup document. A "phonemes" property was described in earlier drafts of this specification, but
      objections were raised due to breaking the principle of separation between content and presentation (the "phonemes" authored within aural CSS stylesheets would have needed to be updated each time text changed within the markup document). The
      "phonemes" functionality is therefore considered out-of-scope in CSS (the presentation layer) and should be addressed in the markup / content layer.
    </blockquote>
    <p>While a portion of CSS Speech was demonstrated by <a href="https://developer.apple.com/videos/play/wwdc2011/519/">Apple in 2011 on iOS with Safari and VoiceOver</a>, it is not presently supported on any platform with any Assistive Technology,
    and work on the standard has itself been stopped by the CSS working group.</p>
    <p>Efforts to address the need for pronunciation standards have also been considered by both assessment technology vendors and the publishing community. Citing the need for pronunciation and presentation controls, the IMS Global Learning Consortium added the ability to author SSML
    markup, specify PLS files, and reference CSS Speech properties to the Question Test Interoperability (QTI) <a href="https://www.imsglobal.org/apip/apipv1p0/APIP_QTI_v1p0.html">Accessible Portable Item Protocol</a> (APIP). In practice, QTI/APIP
    authored content is transformed into HTML for rendering in web browsers. This led to the dilemma that there is no standardized (and supported) method for inlining SSML in HTML, nor is there support for CSS Speech. This has led to the situation
    where SSML is the primary authoring model, with assessment vendors implementing a custom method for adding the SSML (or SSML-like) features to HTML using non-standard or data attributes, with customized Read Aloud software consuming those
    attributes for text to speech synthesis. Given the need to deliver accurate spoken presentation, non-standard approaches often include mis-use of WAI-ARIA, and novel or contextually non-valid attributes (e.g., <code>label</code>). A
    particular problem occurs when custom pronunciation is applied via a misuse of the <code>aria-label</code> attribute, which results in an issue for screen reader users who also rely upon refreshable braille, and in which a hinted pronunciation
    intended only for a text to speech synthesizer also appears on the braille display.</p>
    <p>The attribute model for adding pronunciation and presentation guidance for assistive technologies and text to speech synthesis has demonstrated traction by vendors trying to solve this need. It should be noted that many of the required
    features are not well supported by a single attribute, as most follow the form of a presentation property / value pairing. Using multiple attributes to provide guidance to assistive technologies is not novel, as seen with WAI-ARIA where multiple
    attributes may be applied to a single element, for example, <code>role</code> and <code>aria-checked</code>. The <a href="https://w3c.github.io/publ-epub-revision/epub32/spec/epub-contentdocs.html#sec-xhtml-ssml-attrib">EPUB standard</a> for
    digital publishing introduced a namespaced version of the SSML <code>phoneme</code> and <code>alphabet</code> attributes enabling content authors to provide pronunciation guidance. Uptake by the publishing community has been limited, reportedly
    due to the lack of support in reading systems and assistive technologies.</p>
    <p>In order to further the discussion about the essential need for consistent pronunciation and presentation of content spoken by text 
          to speech synthesis (TTS), the Pronunciation Task Force has compiled the following sections of information in this document:
        <ul>
       <li>Core Features for Pronunciation and Spoken Presentation, which describes and give examples of needed pronunciation supports.</li>
        <li>Use Cases, which provides technical examples of pronunciation supports for 6 approaches and lists the goal of the approach, the 
              , implementation options, existing work, and problems and limitations.</li>
        <li>Gap Analysis, which identifies gaps in support based on the requirements and use cases. Gaps are defined when a requirement does not have a corresponding use case approach by which it can be authored in HTML.</li></ul>
        </p>
  </section>
  <section>
    <h1>Core Features for Pronunciation and Spoken Presentation</h1>
    <p>The common spoken pronunciation requirements from the education domain serve as a primary source for the core features and are applicable to many other domains that require accurate pronunciation and presentation. These requirements can be broken down into the following main functions that would support authoring and spoken presentation
    needs.</p>
    <section>
      <h2>Language</h2>
      <p>When content is authored in mixed language, a mechanism is needed to allow authors to indicate both the base language of the content as well as the language of individual words and phrases. The expectation is that assistive technologies and
    other tools that utilize text to speech synthesis would detect and apply the language requested when presenting the text.</p>
    </section>
    <section>
      <h2>Voice Family / Gender</h2>
      <p>Content authors may elect to adjust the spoken presentation to provide a gender specific voice to reflect that of the author, or for a character (or characters) in theatrical a
      presentation of a story. Many assistive technologies already provide user selection of voice family and gender independent of any authored intent.</p>
    </section>
    <section>
      <h2>Phonetic Pronunciation of String Values</h2>
      <p>In some cases, words may need to have their phonetic pronunciation prescribed by the content author. This may occur when uncommon words (not supported by text to speech synthesizers)are present, or in cases where word pronunciation will vary based on
      context, and that context may not be correctly interpreted.</p>
    </section>
    <section>
      <h2>String Substitution</h2>
      <p>There are cases where content that is visually presented may require replacement (substitution) with an alternate textual form to ensure correct pronunciation by text to speech synthesizers. In these cases, phonetic pronunciation may be a
      solution to this need.</p>
    </section>
    <section>
      <h2>Rate / Pitch / Volume</h2>
      <p>While end users should have full control over spoken presentation parameters such as speaking rate, pitch, and volume (e.g., WCAG 1.4.2 ), content authors may elect to adjust those parameters to control the spoken presentation for
      purposes such as a theatrical presentation of a story. Many assistive technologies already provide user controlled speaking rate, pitch, and volume independent of any authored intent.</p>
    </section>
    <section>
    <h2>Emphasis</h2>
    <p>In written text, an author may find it necessary to add emphasis to an important word or phrase. HTML supports both semantic elements (e.g., <code>em</code>) and CSS properties which, through a variety of style options, make programmatic
    detection of authored emphasis difficult (e.g., <code>font-weight: heavy</code>). While the emphasis element has existed since HTML 2.0, there is currently no uptake by assistive technology or read aloud tools to present text semantically tagged
    for emphasis to be spoken with emphasis.</p>
    </section>
    <section>
    <h2>Say As</h2>
      <p>While text to speech engines continue to improve in their ability to process text and provide accurate spoken rendering of acronyms and numeric values, there can be instances where uncommon terms or alphanumeric constructs pose challenges.</p>
      <section>
        <h3>Presentation of Numeric Values</h3>
        <p>Precise spoken presentation of numeric values may not always be correctly determined by text to speech engines from context.&nbsp; Examples include speaking a multidigit number as individual digits (100 spoken as &quot;one, zer, zero&quot; instead of &quot;one hundred&quot;), correct reading of year values, and
        the correct speaking of ordinal and cardinal numbers. Furthermore, some educators may have specific requirements for the spoken presentation of a numeric value which may differ from a TTS engine's default rendering. For example, the Smarter Balanced Assessment Consortium has developed <a href=
      "https://portal.smarterbalanced.org/library/en/read-aloud-guidelines.pdf">Read Aloud Guidelines</a> to be followed by human readers used by students who may require a spoken presentation of an educational test, which includes specific examples
      of how numeric values should be read aloud.</p>
      </section>
      <section>
        <h3>Presentation of String Values</h3>
        <p>Precise presentation of string values may not be determined correctly by text to speech synthesizers. Examples include speaking acroyms as individual letters rather than words and providing a substitute pronunciation for a word based on context (the word &quot;read&quot; may be pronounced as either &quot;reed&quot; or &quot;red&quot;), and providing a phonetic pronunciation of a word to ensure the correct spoken presentation.</p>
      </section>
    </section>
    <section>
      <h2>Pausing</h2>
      <p>Content authors may elect or find it necessary to insert pauses in content. Pauses can be inserted for dramatic effect or may be necessary to render the spoken presentation understandable. For example, pauses inserted between numeric values may be needed to limit the chance of hearing multiple numbers as a single value. One common technique to achieve pausing to date has involved inserting non-visible commas before or after a text string requiring a pause. While this may work in practice for a read aloud TTS tool, it is problematic for screen reader users who may, based on verbosity settings, hear the multiple commas announced, and for refreshable braille users who will have the commas visible in braille. In addition, some tests, such as PARCC, specify spoken presentation requirements in their accessibility guidelines which include inserting pauses before and after emphasized word and mathematical terms.</p>
    </section>
  </section>
  <section>
    <h1>Use Cases</h1>
    <p>This section presents 6 use cases which describe specific implmentationimplementation approaches for introducing pronunciation and spoken presentation authoring markup into HTML5 and are based on an inline or attribute model. These two differing approach models emerged from the work of the Accessible Platform Architecture Pronunciation Task Force and represent a decision point for integrating SSML (or SSML-like characteristics) into HTML. Each of the approaches differs in authoring and consumption models (specifically for assistive technologies. Other approaches may appear in subsequent working drafts.</p>
    <p>Successful use cases provide ease of authoring and consumption by assistive technologies and user 
        agents that utilize synthetic speech for spoken presentation of web content. The most challenging aspect of consumption may
        be alignment of the markup approach with the standard mechanisms by which assistive technologies, specifically screen
        readers, obtain content via platform accessibility APIs.
    </p>
        <section>
      <h1>Use Case aria-ssml</h1>
      <section>
        <h2>Background and Current Practice</h2>
        <p>A new <code>aria</code> attribute could be used to include pronunciation content.</p>
      </section>
      <section>
        <h2>Goal</h2>
        <p>Embed SSML in an HTML document.</p>
      </section>
      <section>
        <h2>Target Audience</h2>
        <ul>
          <li>Assistive Technology</li>
          <li>Browser Extensions</li>
          <li>Search Engines</li>
        </ul>
      </section>
      <section>
        <h2>Implementation Options</h2>
        <p><strong>aria-ssml as embedded JSON</strong></p>
        <p>When AT encounters an element with aria-ssml, the AT should enhance the UI by processing the pronunciation content and passing it to the <a href="https://w3c.github.io/speech-api/">Web Speech API</a> or an external API (e.g., <a href="https://cloud.google.com/text-to-speech/">Google's Text to Speech API</a>).</p>
        <pre class="example">I say &lt;span aria-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;pɪˈkɑːn&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.
You say &lt;span aria-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;ˈpi.k&aelig;n&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.</pre>
        <p>Client will convert JSON to SSML and pass the XML string a speech API.</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = convertJSONtoSSML(element.getAttribute('aria-ssml'));
speechSynthesis.speak(msg);</pre>
        <p><strong>aria-ssml referencing XML by template ID</strong></p>
        <pre class="example">&lt;!-- ssml must appear inside a template to be valid --&gt;
&lt;template id=&quot;pecan&quot;&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/template&gt;

&lt;p aria-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
        <p>Client will parse XML and serialize it before passing to a speech API:</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
var xml = document.getElementById('pecan').content.firstElementChild;
msg.text = serialize(xml);
speechSynthesis.speak(msg);</pre>

        <p><strong>aria-ssml referencing an XML string as script tag</strong></p>
        <pre class="example">&lt;script id=&quot;pecan&quot; type=&quot;application/ssml+xml&quot;&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/script&gt;

&lt;p aria-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
        <p>Client will pass the XML string raw to a speech API.</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = document.getElementById('pecan').textContent;
speechSynthesis.speak(msg);</pre>
        <p><strong>aria-ssml referencing an external XML document by URL</strong></p>
        <pre class="example">&lt;p aria-ssml=&quot;http://example.com/pronounce.ssml#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
        <p>Client will pass the string payload to a speech API.</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
var response = await fetch(el.dataset.ssml)
msg.txt = await response.text();
speechSynthesis.speak(msg);</pre>
      </section>
      <section>
        <h2>Existing Work</h2>
        <ul>
          <li><a href="https://github.com/alia11y/SSMLinHTMLproposal">aria-ssml proposal</a></li>
          <li><a href="https://www.w3.org/TR/speech-synthesis11/">SSML</a></li>
          <li><a href="https://w3c.github.io/speech-api/">Web Speech API</a></li>
        </ul>
      </section>
      <section>
        <h2>Problems and Limitations</h2>
        <ul>
          <li>aria-ssml is not a valid aria-* attribute.</li>
          <li>OS/Browsers combinations that do not support the serialized XML usage of the Web Speech API.</li>
        </ul>
      </section>
      
    </section>

    <section>
      <h1>Use Case data-ssml</h1>
      <section>
        <h2>Background and Current Practice</h2>
        <p>As an existing attribute, data-* could be used, with some conventions, to include pronunciation content.</p>
      </section>
      <section>
        <h2>Goal</h2>
        <ul>
          <li>Support repeated use within the page context</li>
          <li>Support external file references</li>
          <li>Reuse existing techniques without expanding specifications</li>
        </ul>
      </section>
      <section>
        <h2>Target Audience</h2>
        <p>Hearing users</p>
      </section>
      <section>
        <h2>Implementation Options</h2>
        <p><strong>data-ssml as embedded JSON</strong></p>
        <p>When an element with data-ssml is encountered by an SSML-aware AT, the AT should enhance the user interface by processing the referenced SSML content and passing it to the <a href="https://w3c.github.io/speech-api/">Web Speech API</a> or an external API (e.g., <a href="https://cloud.google.com/text-to-speech/">Google's Text to Speech API</a>).</p>
        <pre class="example">&lt;h2&gt;The Pronunciation of Pecan&lt;/h2&gt;
&lt;p&gt;&lt;speak&gt;
I say &lt;span data-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;pɪˈkɑːn&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.
You say &lt;span data-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;ˈpi.k&aelig;n&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.</pre>
        <p>Client will convert JSON to SSML and pass the XML string a speech API.</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = convertJSONtoSSML(element.dataset.ssml);
speechSynthesis.speak(msg);</pre>

        <p><strong>data-ssml referencing XML by template ID</strong></p>
        <pre class="example">&lt;!-- ssml must appear inside a template to be valid --&gt;
&lt;template id=&quot;pecan&quot;&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/template&gt;

&lt;p data-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
        <p>Client will parse XML and serialize it before passing to a speech API:</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
var xml = document.getElementById('pecan').content.firstElementChild;
msg.text = serialize(xml);
speechSynthesis.speak(msg);</pre>

        <p><strong>data-ssml referencing an XML string as script tag</strong></p>
        <pre class="example">&lt;script id=&quot;pecan&quot; type=&quot;application/ssml+xml&quot;&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/script&gt;

&lt;p data-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
        <p>Client will pass the XML string raw to a speech API.</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = document.getElementById('pecan').textContent;
speechSynthesis.speak(msg);</pre>
        <p><strong>data-ssml referencing an external XML document by URL</strong></p>
        <pre class="example">&lt;p data-ssml=&quot;http://example.com/pronounce.ssml#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
        <p>Client will pass the string payload to a speech API.</p>
        <pre class="example">var msg = new SpeechSynthesisUtterance();
var response = await fetch(el.dataset.ssml)
msg.txt = await response.text();
speechSynthesis.speak(msg);</pre>
      </section>
      <section>
        <h2>Existing Work</h2>
        <ul>
          <li><a href="https://github.com/alia11y/SSMLinHTMLproposal">aria-ssml proposal</a></li>
          <li><a href="https://www.w3.org/TR/speech-synthesis11/">SSML</a></li>
          <li><a href="https://w3c.github.io/speech-api/">Web Speech API</a></li>
        </ul>
      </section>
      <section>
        <h2>Problems and Limitations</h2>
        <ul>
          <li>Does not assume or suggest visual pronunciation help for deaf or hard of hearing</li>
          <li>Use of data-* requires input from AT vendors</li>
          <li>XML data is not indexed by search engines</li>
        </ul>
      </section>
    </section>

    <section>
      <h1>Use Case HTML5</h1>
      <section>
        <h2>Background and Current Practice</h2>
        <p>HTML5 includes the XML namespaces for MathML and SVG. Therefore, using either's elements in an HTML5 document is valid. Likewise, including an SSML namespace would allow the valid use of SSML in HTML5. Because SSML's implementation is non-visual in nature, browser implementation could be slow or non-existent without affecting how authors use SSML in HTML. Browsers would treat the element like any other unknown element, as HTMLUnknownElement.</p>
      </section>
      <section>
        <h2>Goal</h2>
        <ul>
          <li>Support valid use of SSML in HTML5 documents</li>
          <li>Allow visual pronunciation support</li>
        </ul>
      </section>
      <section>
        <h2>Target Audience</h2>
        <ul>
          <li>SSML-aware technologies and browser extensions</li>
          <li>Search indexers</li>
        </ul>
      </section>
      <section>
        <h2>Implementation Options</h2>
        <p><strong>SSML</strong></p>
        <p>When an element with data-ssml is encountered by an <a href="https://www.w3.org/TR/speech-synthesis11/">SSML</a>-aware AT, the AT should enhance the user interface by processing the referenced SSML content and passing it to the <a href="https://w3c.github.io/speech-api/">Web Speech API</a> or an external API (e.g., <a href="https://cloud.google.com/text-to-speech/">Google's Text to Speech API</a>).</p>
        <pre class="example">&lt;h2&gt;The Pronunciation of Pecan&lt;/h2&gt;
  &lt;p&gt;&lt;speak&gt;
  You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
  I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;&lt;/p&gt;</pre>
      </section>
      <section>
        <h2>Existing Work</h2>
        <ul>
          <li><a href="https://www.w3.org/TR/voicexml21/">VoiceXML 2.1</a></li>
          <li><a href="https://www.w3.org/TR/REC-smil/smil-extended-linking.html#SMILLinking-Relationship-to-XLink">SMIL - Synchronized Multimedia Integration Language</a></li>
          <li><a href="https://www.w3.org/TR/pronunciation-lexicon/#AppB">PLS - Pronunciation Lexicon</a></li>
        </ul>
      </section>
      <section>
        <h2>Problems and Limitations</h2>
        <p>SSML is not valid HTML5</p>
      </section>
    </section>

    <section>
      <h1>Use Case Custom Element</h1>
      <section>
        <h2>Background and Current Practice</h2>
        <p>Embed valid SSML in HTML using custom elements registered as ssml-* where * is the actual SSML tag name (except for p which expects the same treatment as an HTML p in HTML layout).</p>
      </section>
      <section>
        <h2>Goal</h2>
        <p>Support use of SSML in HTML documents.</p>
      </section>
      <section>
        <h2>Target Audience</h2>
        <ul>
          <li>SSML-aware technologies and browser extensions</li>
          <li>Search indexers</li>
        </ul>
      </section>
      <section>
        <h2>Implementation Options</h2>
        <p><strong>ssml-speak: see <a href="https://ssml-components.glitch.me">demo</a> </strong></p>
        <p>Only the &lt;ssml-speak&gt; component requires registration. The component code lifts the SSML by getting the innerHTML and removing the ssml- prefix from the interior tags and passing it to the web speech API. The &lt;p&gt; tag from SSML is not given the prefix because we still want to start a semantic paragraph within the content. The other tags used in the example have no semantic meaning. Tags like &lt;em&gt; in HTML could be converted to &lt;emphasis&gt; in SSML. In that case, CSS styles will come from the browser's default styles or the page author.</p>
        <pre class="example">&lt;ssml-speak&gt;
  Here are &lt;ssml-say-as interpret-as=&quot;characters&quot;&gt;SSML&lt;/ssml-say-as&gt; samples.
  I can pause&lt;ssml-break time=&quot;3s&quot;&gt;&lt;/ssml-break&gt;.
  I can speak in cardinals.
  Your number is &lt;ssml-say-as interpret-as=&quot;cardinal&quot;&gt;10&lt;/ssml-say-as&gt;.
  Or I can speak in ordinals.
  You are &lt;ssml-say-as interpret-as=&quot;ordinal&quot;&gt;10&lt;/ssml-say-as&gt; in line.
  Or I can even speak in digits.
  The digits for ten are &lt;ssml-say-as interpret-as=&quot;characters&quot;&gt;10&lt;/ssml-say-as&gt;.
  I can also substitute phrases, like the &lt;ssml-sub alias=&quot;World Wide Web Consortium&quot;&gt;W3C&lt;/ssml-sub&gt;.
  Finally, I can speak a paragraph with two sentences.
  &lt;p&gt;
    &lt;ssml-s&gt;You say, &lt;ssml-phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/ssml-phoneme&gt;.&lt;/ssml-s&gt;
    &lt;ssml-s&gt;I say, &lt;ssml-phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/ssml-phoneme&gt;.&lt;/ssml-s&gt;
  &lt;/p&gt;
&lt;/ssml-speak&gt;
&lt;template id=&quot;ssml-controls&quot;&gt;
  &lt;style&gt;
    [role=&quot;switch&quot;][aria-checked=&quot;true&quot;] :first-child,
    [role=&quot;switch&quot;][aria-checked=&quot;false&quot;] :last-child {
      background: #000;
      color: #fff;
    }
  &lt;/style&gt;
  &lt;slot&gt;&lt;/slot&gt;
  &lt;p&gt;
    &lt;span id=&quot;play&quot;&gt;Speak&lt;/span&gt;
    &lt;button role=&quot;switch&quot; aria-checked=&quot;false&quot; aria-labelledby=&quot;play&quot;&gt;
      &lt;span&gt;on&lt;/span&gt;
      &lt;span&gt;off&lt;/span&gt;
    &lt;/button&gt;
  &lt;/p&gt;
&lt;/template&gt;</pre>
        <pre class="example">class SSMLSpeak extends HTMLElement {
  constructor() {
    super();
    const template = document.getElementById('ssml-controls');
    const templateContent = template.content;
    this.attachShadow({mode: 'open'})
      .appendChild(templateContent.cloneNode(true));
  }
  connectedCallback() {
    const button = this.shadowRoot.querySelector('[role=&quot;switch&quot;][aria-labelledby=&quot;play&quot;]')
    const ssml = this.innerHTML.replace(/ssml-/gm, '')
    const msg = new SpeechSynthesisUtterance();
    msg.lang = document.documentElement.lang;
    msg.text = `&lt;speak version=&quot;1.1&quot;
      xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
      xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
        http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
      xml:lang=&quot;${msg.lang}&quot;&gt;
    ${ssml}
    &lt;/speak&gt;`;
    msg.voice = speechSynthesis.getVoices().find(voice =&gt; voice.lang.startsWith(msg.lang));
    msg.onstart = () =&gt; button.setAttribute('aria-checked', 'true');
    msg.onend = () =&gt; button.setAttribute('aria-checked', 'false');
    button.addEventListener('click', () =&gt; speechSynthesis[speechSynthesis.speaking ? 'cancel' : 'speak'](msg))
  }
}

customElements.define('ssml-speak', SSMLSpeak);</pre>
      </section>
      <section>
        <h2>Existing Work</h2>
        <ul>
          <li><a href="https://dom.spec.whatwg.org/#concept-element-custom">DOM Living Standard</a></li>
          <li><a href="https://w3c.github.io/speech-api/">Web Speech API</a></li>
        </ul>
      </section>
      <section>
        <h2>Problems and Limitations</h2>
        <ul>
          <li>OS/Browsers combinations that do not support the serialized XML usage of the Web Speech API.</li>
          <li>Browsers may need to map SSML tags with CSS styles for default user agent styles.</li>
          <li>Without an extension or AT, only user interaction can start the Web Speech API.</li>
          <li>Authors or parsing may need to remove HTML content with unintended SSML semantics before serialization.</li>
        </ul>
      </section>
    </section>

    <section>
      <h1>Use Case JSON-LD</h1>
      <section>
        <h2>Background and Current Practice</h2>
        <p><a href="https://www.w3.org/2018/jsonld-cg-reports/json-ld/">JSON-LD</a> provides an established standard for embedding data in HTML. Unlike other microdata approaches, JSON-LD helps to reuse standardized annotations through external references.</p>
      </section>
      <section>
        <h2>Goal</h2>
        <p>Support use of SSML in HTML documents.</p>
      </section>
      <section>
        <h2>Target Audience</h2>
        <ul>
          <li>SSML-aware technologies and browser extensions</li>
          <li>Search indexers</li>
        </ul>
      </section>
      <section>
        <h2>Implementation Options</h2>
        <p><strong>JSON-LD</strong></p>
        <pre class="example">&lt;script type=&quot;application/ld+json&quot;&gt;
{
  &quot;@context&quot;: &quot;http://schema.org/&quot;,
  &quot;@id&quot;: &quot;/Pronunciation#WKRP&quot;,
  &quot;@type&quot;: &quot;TextPronunciation&quot;,
  &quot;@language&quot;: &quot;en&quot;,
  &quot;text&quot;: &quot;WKRP&quot;,
  &quot;speechToTextMarkup&quot;: &quot;SSML&quot;,
  &quot;phoneticText&quot;: &quot;&lt;say-as interpret-as=\&quot;characters\&quot;&gt;WKRP&lt;/say-as&gt;&quot;
}
&lt;/script&gt;
&lt;p&gt;
  Do you listen to &lt;span itemscope
    itemtype=&quot;http://schema.org/TextPronunciation&quot;
    itemid=&quot;/Pronunciation#WKRP&quot;&gt;WKRP&lt;/span&gt;?
&lt;/p&gt;</pre>
      </section>
      <section>
        <h2>Existing Work</h2>
        <ul>
          <li><a href="https://www.w3.org/WoT/WG/">Web of Things Working Group</a></li>
          <li><a href="https://schema.org/">Schema.org</a></li>
        </ul>
      </section>
      <section>
        <h2>Problems and Limitations</h2>
        <p>not an established "type"/published schema</p>
      </section>
    </section>
    <section>
      <h1>Use Case Ruby</h1>
      <section>
        <h2>Background and Current Practice</h2>
        <blockquote>&lt;Ruby&gt; annotations are short runs of text presented alongside base text, primarily used in East Asian typography as a guide for pronunciation or to include other annotations.</blockquote>
        <p>ruby guides pronunciation visually. This seems like a natural fit for text-to-speech.</p>
      </section>
      <section>
        <h2>Goal</h2>
        <ul>
          <li>Support use of SSML in HTML documents.</li>
          <li>Offer visual pronunciation support.</li>
        </ul>
      </section>
      <section>
        <h2>Target Audience</h2>
        <ul>
          <li>AT and browser extensions</li>
          <li>Search indexers</li>
        </ul>
      </section>
      <section>
        <h2>Implementation Options</h2>
        <p><strong>ruby with microdata</strong></p>
        <p>Microdata can augment the ruby element and its descendants.</p>
        <pre class="example">&lt;p&gt;
  You say,
  &lt;span itemscope=&quot;&quot; itemtype=&quot;http://example.org/Pronunciation&quot;&gt;
    &lt;ruby itemprop=&quot;phoneme&quot; content=&quot;pecan&quot;&gt;
      pecan
      &lt;rt itemprop=&quot;ph&quot;&gt;pɪˈkɑːn&lt;/rt&gt;
      &lt;meta itemprop=&quot;alphabet&quot; content=&quot;ipa&quot;&gt;
    &lt;/ruby&gt;.
  &lt;/span&gt;
  I say,
  &lt;span itemscope=&quot;&quot; itemtype=&quot;http://example.org/Pronunciation&quot;&gt;
    &lt;ruby itemprop=&quot;phoneme&quot; content=&quot;pecan&quot;&gt;
      pe
      &lt;rt itemprop=&quot;ph&quot;&gt;ˈpi&lt;/rt&gt;
      can
      &lt;rt itemprop=&quot;ph&quot;&gt;k&aelig;n&lt;/rt&gt;
      &lt;meta itemprop=&quot;alphabet&quot; content=&quot;ipa&quot;&gt;
    &lt;/ruby&gt;.
  &lt;/span&gt;
&lt;/p&gt;</pre>
      </section>
      <section>
        <h2>Existing Work</h2>
        <ul>
          <li><a href="https://html.spec.whatwg.org/multipage/text-level-semantics.html#the-ruby-element">HTML Living Standard</a></li>
          <li><a href="https://schema.org/">Schema.org</a></li>
        </ul>
      </section>
      <section>
        <h2>Problems and Limitations</h2>
        <ul>
          <li>AT may process annotations as content</li>
          <li>AT "double reading" words instead of choosing either the content or the annotation</li>
          <li>Only offers for a few SSML expressions</li>
          <li>Difficult to reuse by reference</li>
        </ul>
      </section>
    </section>
  </section>
  <section>
    <h1>Gap Analysis</h1>
    <p>Based on the features and use cases described in the prior sections, the following table presents existing speech presentation standards, HTML features, and WAI-ARIA attributes that may offer a method to achieve the requirement for HTML authors. A blank cell for any approach represents a gap in support.</p>
    <table border="1"
           cellpadding="2"
           cellspacing="2"
           width="100%">
      <tbody>
        <tr>
          <th align="left"
              valign="top">Requirement<br /></th>
          <th valign="top">HTML<br /></th>
          <th valign="top">WAI-ARIA<br /></th>
          <th valign="top">PLS<br /></th>
          <th valign="top">CSS Speech<br /></th>
          <th valign="top">SSML<br /></th>
        </tr>
        <tr>
          <th align="left"
              valign="top">Language<br /></th>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Voice Family/Gender<br /></th>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Phonetic Pronunciation<br /></th>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Substitution<br /></th>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Partial<br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Rate/Pitch/Volume<br /></th>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Emphasis<br /></th>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Say As<br /></th>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
        <tr>
          <th align="left"
              valign="top">Pausing<br /></th>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top"><br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
          <td align="center"
              valign="top">Yes<br /></td>
        </tr>
      </tbody>
    </table>
    <p>The following sections describe how each of the required features may be met by the use of existing approaches. A key consideration in the analysis is whether a means exists to directly author (or annotate) HTML content to incorporate the
    spoken presentation and pronunciation feature.</p>
    <section>
      <h3>Language</h3>
      <p>Allow content authors to specify the language of text contained within an element so that the TTS used for rendering will select the appropriate language for synthesis.<br /></p>
    
      <h4>HTML</h4>
      <p><code>lang</code> attribute can be applied at the document level or to individual elements. (WCAG) (AT Supported: some)<br /></p>
      <h4>SSML</h4>
      <p>Example: <code>&lt;speak&gt; In Paris, they pronounce it &lt;lang xml:lang="fr-FR"&gt;Paris&lt;/lang&gt; &lt;/speak&gt;</code>code></p>
    </section>
    <section>
      <h3>Voice Family/Gender</h3>
      <p>Allow content authors to specify a specific TTS voice to be used to render text. For example, for content that presents a dialog between two people, a woman and a man, the author may specify that a female voice be used for the woman's text and a
      male voice be used for the man's text. Some platform TTS services may support a variety of voices, identified by a name, gender, or even age.</p>
      <h4>CSS</h4>
      <p><code>voice-family</code> property can be used to specify the gender of the voice.<br /></p>
      <p>Example: <code>{ voice-family: male; }</code></p>
      <h4>SSML</h4>
      <p>Using the <code>&lt;voice&gt;</code> element, the gender of the speaker, if supported by the TTS engine, can be specified.</p>
      <p>Example: <code>&lt;voice gender="female" &gt;Mary had a little lamb,&lt;/voice&gt;</code></p>

    </section>
    <section>
      <h3>Phonetic Pronunciation</h3>
      <p>Allow content authors to precisely specify the phonetic pronunciation of a word or phrase.</p>
      <h4>PLS</h4>
         <p>Using PLS, all the pronunciations can be factored out into an external PLS document which is referenced by the <code>&lt;lexicon&gt;</code> element of SSML </p>
          <p>
                <pre class="example">Example: <code>&lt;speak&gt; &lt;lexicon uri="http://www.example.com/movie_lexicon.pls"/&gt;
                The title of the movie is: "La vita è bella" (Life is beautiful),
    which is directed by Roberto Benigni.&lt;/speak&gt;</code>
    </pre>
    </p>     
      
          <h4>SSML</h4>
          <p>The following is a simple example of an SSML document. It includes an Italian movie title and the name of the director to be read in US English.</p>
      <p>
            <pre class="example">Example: The title of the movie is: 
            <code>&lt;speak&gt; &lt;phoneme alphabet="ipa" ph="ˈlɑ ˈviːɾə ˈʔeɪ ˈbɛlə"&gt;
            "La vita è bella"&lt;/phoneme&gt; (Life is beautiful),
            which is directed by 
            &lt;phoneme alphabet="ipa" ph="ɹəˈbɛːɹɾoʊ bɛˈniːnji""&gt; 
            Roberto Benigni &lt;/phoneme&gt;.&lt;/speak&gt;
            </code>
            </pre>
        </p>     
          
    </section>
    <section>
      <h3>Substitution</h3>
      <p>Allow content authors to substitute a text string to be rendered by TTS instead of the actual text contained in an element.</p>
      <h4>WAI-ARIA</h4>
      <p>The <a href="https://www.w3.org/TR/wai-aria/#aria-label"><code>aria-label</code></a> and <a href="https://www.w3.org/TR/wai-aria/#aria-labelledby"><code>aria-labelledby</code></a> attribute can be used by an author to supply a text string
      that will become the accessible name for the element upon which it is applied.&nbsp; This usage effectively provides a mechanism for performing text substation that is supported by a screen reader. However, it is problematic for one significant reason; for users who utilize screen readers and refreshable Braille, the content that is voiced will not match the content that is sent to the refreshable Braille device. This mismatch would not be acceptable for some content, particularly for assessment content.<br /></p>
      <h4>SSML</h4>
          <p>Pronounce the specified word or phrase as a different word or phrase. Specify the pronunciation to substitute with the <code>alias</code> attribute.</p>
         <p> <pre class="example">
           <code>
              &lt;speak&gt;
                  My favorite chemical element is &lt;sub alias="aluminum"&gt;Al&lt;/sub&gt;,
                  but Al prefers &lt;sub alias="magnesium">Mg&lt;/sub&gt;. 
            &lt;/speak&gt;
            </code>
          </pre>
          </p>   
    </section>
    <section>
      <h3>Rate/Pitch/Volume</h3>
      <p>Allow content authors to specify characteristics, such as rate, pitch, and/or volume of the TTS rendering of the text.</p>
      <h4>CSS</h4>
          <p>
           <dl>
                 <dt><strong>voice-rate</strong></dt>
                 <dd>The <code>‘voice-rate’</code> property manipulates the rate of generated synthetic speech in terms of words per minute.</dd>
            </dl>          
          </p>

         <p>
           <dl>
                 <dt><strong>voice-pitch</strong></dt>
                 <dd>The <code>‘voice-pitch’</code> property specifies the "baseline" pitch of the generated speech output, which depends on the used <code>‘voice-family’</code> instance, and varies across speech synthesis processors (it approximately corresponds to the average pitch of the output). For example, the common pitch for a male voice is around 120Hz, whereas it is around 210Hz for a female voice.</dd>
            </dl>          
          </p>
        <p>
           <dl>
                 <dt><strong>voice-range</strong></dt>
                 <dd>The <code>‘voice-range’</code> property specifies the variability in the "baseline" pitch, i.e. how much the fundamental frequency may deviate from the average pitch of the speech output. The dynamic pitch range of the generated speech generally increases for a highly animated voice, for example when variations in inflection are used to convey meaning and emphasis in speech. Typically, a low range produces a flat, monotonic voice, whereas a high range produces an animated voice.</dd>
            </dl>          
          </p>
      <h4>SSML</h4>
          <p><code>prosody</code> modifies the volume, pitch, and rate of the tagged speech.</p>
          <p>
          <pre class="example">
            <code>
              &lt;speak&gt;
                  Normal volume for the first sentence.
                  &lt;prosody volume="x-loud"&gt;Louder volume for the second sentence&lt;/prosody&gt;.
                  When I wake up, &lt;prosody rate="x-slow"&gt;I speak quite slowly&lt;/prosody&gt;.
                  I can speak with my normal pitch, 
                  &lt;prosody pitch="x-high"&gt; but also with a much higher pitch &lt;/prosody&gt;, 
                  and also &lt;prosody pitch="low"&gt;with a lower pitch&lt;/prosody&gt;.
            &lt;/speak&gt;
            </code>
          </pre>
          </p>
    </section>
    <section>
      <h3>Emphasis</h3>
      <p>Allow content authors to specify that text content be spoken with emphasis, for example, louder and more slowly. This can be viewed as a simplification of the Rate/Pitch/Volume controls to reduce authoring complexity.</p>
      <h4>HTML</h4>
          <p>
                The HTML <code>&lt;em&gt;</code> element marks text that has stress emphasis. The <code>&lt;em&gt;</code> element can be nested, with each level of nesting indicating a greater degree of emphasis.
          </p>
           <p>
                 The <code>&lt;em&gt;</code> element is for words that have a stressed emphasis compared to surrounding text, which is often limited to a word or words of a sentence and affects the meaning of the sentence itself.

 Typically this element is displayed in italic type. However, it should not be used simply to apply italic styling; use the CSS <code>font-style</code> property for that purpose. Use the <code>&lt;cite&gt;</code> element to mark the title of a work (book, play, song, etc.). Use the <code>&lt;i&gt;</code> element to mark text that is in an alternate tone or mood, which covers many common situations for italics such as scientific names or words in other languages. Use the <code>&lt;strong&gt;</code> element to mark text that has greater importance than surrounding text. 
                </p>      
          
          <h4>CSS</h4>
          <p>
           <dl>
                 <dt><strong>voice-stress</strong></dt>
                 <dd>The <code>‘voice-stress’</code> property manipulates the strength of emphasis, which is normally applied using a combination of pitch change, timing changes, loudness and other acoustic differences. The precise meaning of the values therefore depend on the language being spoken.</dd>
            </dl>          
          </p>
          
      <h4>SSML</h4>
      <p>Emphasize the tagged words or phrases. Emphasis changes rate and volume of the speech. More emphasis is spoken louder and slower. Less emphasis is quieter and faster.</p>         
       <p>
                 <pre class="example">
                 <code>
                         &lt;speak&gt;
                         I already told you I 
                        &lt;emphasis level="strong"&gt;really like&lt;/emphasis&gt; that person.
                        &lt;/speak&gt; 
                 </code>
                 </pre>          
       </p>           
    </section>
    <section>
      <h3>Say As</h3>
      <h4>CSS</h4>
      <h4>SSML</h4>
    </section>
    <section>
      <h3>Pausing</h3>
      <h4>CSS</h4>
      <h4>SSML</h4>
    </section>
  </section>
  <div data-include="../common/acknowledgements.html"
       data-include-replace="true"
       data-oninclude="fixIncludes">
    Acknowledgements placeholder
  </div>
</body>
</html>

<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<title>Pronunciation Use Cases</title>
		<meta charset="utf-8"/>
		<script src="https://www.w3.org/Tools/respec/respec-w3c-common" class="remove"></script>
		
		<script src="respec-config.js" class="remove"></script>

	</head>
	<body>

		<section id="abstract">
            
            
			<p>The objective of the Pronunciation Task Force is to develop normative specifications and best practices guidance collaborating with other W3C groups as appropriate, to provide for proper pronunciation in HTML content when using text to speech (TTS) synthesis. This document provides various use cases highlighting the need for standardization of pronunciation markup, to ensure that consistent and accurate representation of the content. The requirements from the user scenarios provide the basis for these technical requirements/specifications.</p>
		</section>

		<section id="sotd"></section>

		<section class="informative" id="introduction">

			<h1>Introduction</h1>
     
            <p>This document provides use cases which describe specific implmentation approaches for introducing pronunciation
				and spoken presentation authoring markup into HTML5. These approaches are based on the two primary approaches 
				that have evolved from the Pronunciation Task Force members.  Other approaches may appear in subsequent working drafts.
			</p>
			<p>Successful use cases will be those that provide ease of authoring and consumption by assistive technologies and user 
				agents that utilize synthetic speech for spoken presentation of web content. The most challenging aspect of consumption may
				be alignment of the markup approach with the standard mechanisms by which assistive technologies, specifically screen
				readers, obtain content via platform accessibility APIs.
			</p>
			

		</section>
		<section>
			<h1>Use Case aria-ssml</h1>
			<section>
				<h2>Background and Current Practice</h2>
				<p>A new <code>aria</code> attribute could be used to include pronunciation content.</p>
			</section>
			<section>
				<h2>Goal</h2>
				<p>Embed SSML in an HTML document.</p>
			</section>
			<section>
				<h2>Target Audience</h2>
				<ul>
					<li>Assistive Technology</li>
					<li>Browser Extensions</li>
					<li>Search Engines</li>
				</ul>
			</section>
			<section>
				<h2>Implementation Options</h2>
				<p><strong>aria-ssml as embedded JSON</strong></p>
				<p>When AT encounters an element with aria-ssml, the AT should enhance the UI by processing the pronunciation content and passing it to the <a href="https://w3c.github.io/speech-api/">Web Speech API</a> or an external API (e.g., <a href="https://cloud.google.com/text-to-speech/">Google's Text to Speech API</a>).</p>
				<pre class="example">I say &lt;span aria-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;pɪˈkɑːn&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.
You say &lt;span aria-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;ˈpi.k&aelig;n&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.</pre>
				<p>Client will convert JSON to SSML and pass the XML string a speech API.</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = convertJSONtoSSML(element.getAttribute('aria-ssml'));
speechSynthesis.speak(msg);</pre>
				<p><strong>aria-ssml referencing XML by template ID</strong></p>
				<pre class="example">&lt;!-- ssml must appear inside a template to be valid --&gt;
&lt;template id=&quot;pecan&quot;&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/template&gt;

&lt;p aria-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
				<p>Client will parse XML and serialize it before passing to a speech API:</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
var xml = document.getElementById('pecan').content.firstElementChild;
msg.text = serialize(xml);
speechSynthesis.speak(msg);</pre>

				<p><strong>aria-ssml referencing an XML string as script tag</strong></p>
				<pre class="example">&lt;script id=&quot;pecan&quot; type=&quot;application/ssml+xml&quot;&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/script&gt;

&lt;p aria-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
				<p>Client will pass the XML string raw to a speech API.</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = document.getElementById('pecan').textContent;
speechSynthesis.speak(msg);</pre>
				<p><strong>aria-ssml referencing an external XML document by URL</strong></p>
				<pre class="example">&lt;p aria-ssml=&quot;http://example.com/pronounce.ssml#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
				<p>Client will pass the string payload to a speech API.</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
var response = await fetch(el.dataset.ssml)
msg.txt = await response.text();
speechSynthesis.speak(msg);</pre>
			</section>
			<section>
				<h2>Existing Work</h2>
				<ul>
					<li><a href="https://github.com/alia11y/SSMLinHTMLproposal">aria-ssml proposal</a></li>
					<li><a href="https://www.w3.org/TR/speech-synthesis11/">SSML</a></li>
					<li><a href="https://w3c.github.io/speech-api/">Web Speech API</a></li>
				</ul>
			</section>
			<section>
				<h2>Problems and Limitations</h2>
				<ul>
					<li>aria-ssml is not a valid aria-* attribute.</li>
					<li>OS/Browsers combinations that do not support the serialized XML usage of the Web Speech API.</li>
				</ul>
			</section>
			
		</section>

		<section>
			<h1>Use Case data-ssml</h1>
			<section>
				<h2>Background and Current Practice</h2>
				<p>As an existing attribute, data-* could be used, with some conventions, to include pronunciation content.</p>
			</section>
			<section>
				<h2>Goal</h2>
				<ul>
					<li>Support repeated use within the page context</li>
					<li>Support external file references</li>
					<li>Reuse existing techniques without expanding specifications</li>
				</ul>
			</section>
			<section>
				<h2>Target Audience</h2>
				<p>Hearing users</p>
			</section>
			<section>
				<h2>Implementation Options</h2>
				<p><strong>data-ssml as embedded JSON</strong></p>
				<p>When an element with data-ssml is encountered by an SSML-aware AT, the AT should enhance the user interface by processing the referenced SSML content and passing it to the <a href="https://w3c.github.io/speech-api/">Web Speech API</a> or an external API (e.g., <a href="https://cloud.google.com/text-to-speech/">Google's Text to Speech API</a>).</p>
				<pre class="example">&lt;h2&gt;The Pronunciation of Pecan&lt;/h2&gt;
&lt;p&gt;&lt;speak&gt;
I say &lt;span data-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;pɪˈkɑːn&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.
You say &lt;span data-ssml='{&quot;phoneme&quot;:{&quot;ph&quot;:&quot;ˈpi.k&aelig;n&quot;,&quot;alphabet&quot;:&quot;ipa&quot;}}'&gt;pecan&lt;/span&gt;.</pre>
				<p>Client will convert JSON to SSML and pass the XML string a speech API.</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = convertJSONtoSSML(element.dataset.ssml);
speechSynthesis.speak(msg);</pre>

				<p><strong>data-ssml referencing XML by template ID</strong></p>
				<pre class="example">&lt;!-- ssml must appear inside a template to be valid --&gt;
&lt;template id=&quot;pecan&quot;&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/template&gt;

&lt;p data-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
				<p>Client will parse XML and serialize it before passing to a speech API:</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
var xml = document.getElementById('pecan').content.firstElementChild;
msg.text = serialize(xml);
speechSynthesis.speak(msg);</pre>

				<p><strong>data-ssml referencing an XML string as script tag</strong></p>
				<pre class="example">&lt;script id=&quot;pecan&quot; type=&quot;application/ssml+xml&quot;&gt;
&lt;speak version=&quot;1.1&quot;
       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
                   http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
       xml:lang=&quot;en-US&quot;&gt;
    You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
    I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;
&lt;/script&gt;

&lt;p data-ssml=&quot;#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
				<p>Client will pass the XML string raw to a speech API.</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
msg.text = document.getElementById('pecan').textContent;
speechSynthesis.speak(msg);</pre>
				<p><strong>data-ssml referencing an external XML document by URL</strong></p>
				<pre class="example">&lt;p data-ssml=&quot;http://example.com/pronounce.ssml#pecan&quot;&gt;You say, pecan. I say, pecan.&lt;/p&gt;</pre>
				<p>Client will pass the string payload to a speech API.</p>
				<pre class="example">var msg = new SpeechSynthesisUtterance();
var response = await fetch(el.dataset.ssml)
msg.txt = await response.text();
speechSynthesis.speak(msg);</pre>
			</section>
			<section>
				<h2>Existing Work</h2>
				<ul>
					<li><a href="https://github.com/alia11y/SSMLinHTMLproposal">aria-ssml proposal</a></li>
					<li><a href="https://www.w3.org/TR/speech-synthesis11/">SSML</a></li>
					<li><a href="https://w3c.github.io/speech-api/">Web Speech API</a></li>
				</ul>
			</section>
			<section>
				<h2>Problems and Limitations</h2>
				<ul>
					<li>Does not assume or suggest visual pronunciation help for deaf or hard of hearing</li>
					<li>Use of data-* requires input from AT vendors</li>
					<li>XML data is not indexed by search engines</li>
				</ul>
			</section>
		</section>

		<section>
			<h1>Use Case HTML5</h1>
			<section>
				<h2>Background and Current Practice</h2>
				<p>HTML5 includes the XML namespaces for MathML and SVG. So, using either's elements in an HTML5 document is valid. Because SSML's implementation is non-visual in nature, browser implementation could be slow or non-existent without affecting how authors use SSML in HTML. Expansion of HTML5 to include SSML namespace would allow valid use of SSML in the HTML5 document. Browsers would treat the element like any other unknown element, as HTMLUnknownElement.</p>
			</section>
			<section>
				<h2>Goal</h2>
				<ul>
					<li>Support valid use of SSML in HTML5 documents</li>
					<li>Allow visual pronunciation support</li>
				</ul>
			</section>
			<section>
				<h2>Target Audience</h2>
				<ul>
					<li>SSML-aware technologies and browser extensions</li>
					<li>Search indexers</li>
				</ul>
			</section>
			<section>
				<h2>Implementation Options</h2>
				<p><strong>SSML</strong></p>
				<p>When an element with data-ssml is encountered by an <a href="https://www.w3.org/TR/speech-synthesis11/">SSML</a>-aware AT, the AT should enhance the user interface by processing the referenced SSML content and passing it to the <a href="https://w3c.github.io/speech-api/">Web Speech API</a> or an external API (e.g., <a href="https://cloud.google.com/text-to-speech/">Google's Text to Speech API</a>).</p>
				<pre class="example">&lt;h2&gt;The Pronunciation of Pecan&lt;/h2&gt;
  &lt;p&gt;&lt;speak&gt;
  You say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/phoneme&gt;.
  I say, &lt;phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/phoneme&gt;.
&lt;/speak&gt;&lt;/p&gt;</pre>
			</section>
			<section>
				<h2>Existing Work</h2>
				<ul>
					<li><a href="https://www.w3.org/TR/voicexml21/">VoiceXML 2.1</a></li>
					<li><a href="https://www.w3.org/TR/REC-smil/smil-extended-linking.html#SMILLinking-Relationship-to-XLink">SMIL - Synchronized Multimedia Integration Language</a></li>
					<li><a href="https://www.w3.org/TR/pronunciation-lexicon/#AppB">PLS - Pronunciation Lexicon</a></li>
				</ul>
			</section>
			<section>
				<h2>Problems and Limitations</h2>
				<p>SSML is not valid HTML5</p>
			</section>
		</section>

		<section>
			<h1>Use Case Custom Element</h1>
			<section>
				<h2>Background and Current Practice</h2>
				<p>Embed valid SSML in HTML using custom elements registered as ssml-* where * is the actual SSML tag name (except for p which expects the same treatment as an HTML p in HTML layout).</p>
			</section>
			<section>
				<h2>Goal</h2>
				<p>Support use of SSML in HTML documents.</p>
			</section>
			<section>
				<h2>Target Audience</h2>
				<ul>
					<li>SSML-aware technologies and browser extensions</li>
					<li>Search indexers</li>
				</ul>
			</section>
			<section>
				<h2>Implementation Options</h2>
				<p><strong>ssml-speak: see <a href="https://ssml-components.glitch.me">demo</a> </strong></p>
				<p>Only the &lt;ssml-speak&gt; component requires registration. The component code lifts the SSML by getting the innerHTML and removing the ssml- prefix from the interior tags and passing it to the web speech API. The &lt;p&gt; tag from SSML is not given the prefix because we still want to start a semantic paragraph within the content. The other tags used in the example have no semantic meaning. Tags like &lt;em&gt; in HTML could be converted to &lt;emphasis&gt; in SSML. In that case, CSS styles will come from the browser's default styles or the page author.</p>
				<pre class="example">&lt;ssml-speak&gt;
  Here are &lt;ssml-say-as interpret-as=&quot;characters&quot;&gt;SSML&lt;/ssml-say-as&gt; samples.
  I can pause&lt;ssml-break time=&quot;3s&quot;&gt;&lt;/ssml-break&gt;.
  I can speak in cardinals.
  Your number is &lt;ssml-say-as interpret-as=&quot;cardinal&quot;&gt;10&lt;/ssml-say-as&gt;.
  Or I can speak in ordinals.
  You are &lt;ssml-say-as interpret-as=&quot;ordinal&quot;&gt;10&lt;/ssml-say-as&gt; in line.
  Or I can even speak in digits.
  The digits for ten are &lt;ssml-say-as interpret-as=&quot;characters&quot;&gt;10&lt;/ssml-say-as&gt;.
  I can also substitute phrases, like the &lt;ssml-sub alias=&quot;World Wide Web Consortium&quot;&gt;W3C&lt;/ssml-sub&gt;.
  Finally, I can speak a paragraph with two sentences.
  &lt;p&gt;
    &lt;ssml-s&gt;You say, &lt;ssml-phoneme alphabet=&quot;ipa&quot; ph=&quot;pɪˈkɑːn&quot;&gt;pecan&lt;/ssml-phoneme&gt;.&lt;/ssml-s&gt;
    &lt;ssml-s&gt;I say, &lt;ssml-phoneme alphabet=&quot;ipa&quot; ph=&quot;ˈpi.k&aelig;n&quot;&gt;pecan&lt;/ssml-phoneme&gt;.&lt;/ssml-s&gt;
  &lt;/p&gt;
&lt;/ssml-speak&gt;
&lt;template id=&quot;ssml-controls&quot;&gt;
  &lt;style&gt;
    [role=&quot;switch&quot;][aria-checked=&quot;true&quot;] :first-child,
    [role=&quot;switch&quot;][aria-checked=&quot;false&quot;] :last-child {
      background: #000;
      color: #fff;
    }
  &lt;/style&gt;
  &lt;slot&gt;&lt;/slot&gt;
  &lt;p&gt;
    &lt;span id=&quot;play&quot;&gt;Speak&lt;/span&gt;
    &lt;button role=&quot;switch&quot; aria-checked=&quot;false&quot; aria-labelledby=&quot;play&quot;&gt;
      &lt;span&gt;on&lt;/span&gt;
      &lt;span&gt;off&lt;/span&gt;
    &lt;/button&gt;
  &lt;/p&gt;
&lt;/template&gt;</pre>
				<pre class="example">class SSMLSpeak extends HTMLElement {
  constructor() {
    super();
    const template = document.getElementById('ssml-controls');
    const templateContent = template.content;
    this.attachShadow({mode: 'open'})
      .appendChild(templateContent.cloneNode(true));
  }
  connectedCallback() {
    const button = this.shadowRoot.querySelector('[role=&quot;switch&quot;][aria-labelledby=&quot;play&quot;]')
    const ssml = this.innerHTML.replace(/ssml-/gm, '')
    const msg = new SpeechSynthesisUtterance();
    msg.lang = document.documentElement.lang;
    msg.text = `&lt;speak version=&quot;1.1&quot;
      xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;
      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
      xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis
        http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;
      xml:lang=&quot;${msg.lang}&quot;&gt;
    ${ssml}
    &lt;/speak&gt;`;
    msg.voice = speechSynthesis.getVoices().find(voice =&gt; voice.lang.startsWith(msg.lang));
    msg.onstart = () =&gt; button.setAttribute('aria-checked', 'true');
    msg.onend = () =&gt; button.setAttribute('aria-checked', 'false');
    button.addEventListener('click', () =&gt; speechSynthesis[speechSynthesis.speaking ? 'cancel' : 'speak'](msg))
  }
}

customElements.define('ssml-speak', SSMLSpeak);</pre>
			</section>
			<section>
				<h2>Existing Work</h2>
				<ul>
					<li><a href="https://dom.spec.whatwg.org/#concept-element-custom">DOM Living Standard</a></li>
					<li><a href="https://w3c.github.io/speech-api/">Web Speech API</a></li>
				</ul>
			</section>
			<section>
				<h2>Problems and Limitations</h2>
				<ul>
					<li>OS/Browsers combinations that do not support the serialized XML usage of the Web Speech API.</li>
					<li>Browsers may need to map SSML tags with CSS styles for default user agent styles.</li>
					<li>Without an extension or AT, only user interaction can start the Web Speech API.</li>
					<li>Authors or parsing may need to remove HTML content with unintended SSML semantics before serialization.</li>
				</ul>
			</section>
		</section>

		<section>
			<h1>Use Case JSON-LD</h1>
			<section>
				<h2>Background and Current Practice</h2>
				<p><a href="https://www.w3.org/2018/jsonld-cg-reports/json-ld/">JSON-LD</a> provides an established standard for embedding data in HTML. Unlike other microdata approaches, JSON-LD helps to reuse standardized annotations through external references.</p>
			</section>
			<section>
				<h2>Goal</h2>
				<p>Support use of SSML in HTML documents.</p>
			</section>
			<section>
				<h2>Target Audience</h2>
				<ul>
					<li>SSML-aware technologies and browser extensions</li>
					<li>Search indexers</li>
				</ul>
			</section>
			<section>
				<h2>Implementation Options</h2>
				<p><strong>JSON-LD</strong></p>
				<pre class="example">&lt;script type=&quot;application/ld+json&quot;&gt;
{
  &quot;@context&quot;: &quot;http://schema.org/&quot;,
  &quot;@id&quot;: &quot;/pronunciation#WKRP&quot;,
  &quot;@type&quot;: &quot;RadioStation&quot;,
  &quot;name&quot;: [&quot;WKRP&quot;,
    &quot;@type&quot;: &quot;PronounceableText&quot;,
    &quot;textValue&quot;: &quot;WKRP&quot;,
    &quot;speechToTextMarkup&quot;: &quot;SSML&quot;,
    &quot;phoneticText&quot;: &quot;&lt;speak&gt;&lt;say-as interpret-as=\&quot;characters\&quot;&gt;WKRP&lt;/say-as>&quot;
  ]
}
&lt;/script&gt;
&lt;p&gt;
  Do you listen to &lt;span itemscope
    itemtype=&quot;http://schema.org/PronounceableText&quot;
    itemid=&quot;/pronunciation#WKRP&quot;&gt;WKRP&lt;/span&gt;?
&lt;/p&gt;</pre>
			</section>
			<section>
				<h2>Existing Work</h2>
				<ul>
					<li>
						<a href="https://www.w3.org/WoT/WG/">Web of Things Working Group</a>
					</li>
					<li>
						<a href="https://webschemas.org/PronounceableText">Schema.org PronounceableText</a>
					</li>
				</ul>
			</section>
			<section>
				<h2>Problems and Limitations</h2>
				<p>not an established "type"/published schema</p>
			</section>
		</section>
		<section>
			<h1>Use Case Ruby</h1>
			<section>
				<h2>Background and Current Practice</h2>
				<blockquote>&lt;Ruby&gt; annotations are short runs of text presented alongside base text, primarily used in East Asian typography as a guide for pronunciation or to include other annotations.</blockquote>
				<p>ruby guides pronunciation visually. This seems like a natural fit for text-to-speech.</p>
			</section>
			<section>
				<h2>Goal</h2>
				<ul>
					<li>Support use of SSML in HTML documents.</li>
					<li>Offer visual pronunciation support.</li>
				</ul>
			</section>
			<section>
				<h2>Target Audience</h2>
				<ul>
					<li>AT and browser extensions</li>
					<li>Search indexers</li>
				</ul>
			</section>
			<section>
				<h2>Implementation Options</h2>
				<p><strong>ruby with microdata</strong></p>
				<p>Microdata can augment the ruby element and its descendants.</p>
				<pre class="example">&lt;p&gt;
  You say,
  &lt;span itemscope=&quot;&quot; itemtype=&quot;http://example.org/Pronunciation&quot;&gt;
    &lt;ruby itemprop=&quot;phoneme&quot; content=&quot;pecan&quot;&gt;
      pecan
      &lt;rt itemprop=&quot;ph&quot;&gt;pɪˈkɑːn&lt;/rt&gt;
      &lt;meta itemprop=&quot;alphabet&quot; content=&quot;ipa&quot;&gt;
    &lt;/ruby&gt;.
  &lt;/span&gt;
  I say,
  &lt;span itemscope=&quot;&quot; itemtype=&quot;http://example.org/Pronunciation&quot;&gt;
    &lt;ruby itemprop=&quot;phoneme&quot; content=&quot;pecan&quot;&gt;
      pe
      &lt;rt itemprop=&quot;ph&quot;&gt;ˈpi&lt;/rt&gt;
      can
      &lt;rt itemprop=&quot;ph&quot;&gt;k&aelig;n&lt;/rt&gt;
      &lt;meta itemprop=&quot;alphabet&quot; content=&quot;ipa&quot;&gt;
    &lt;/ruby&gt;.
  &lt;/span&gt;
&lt;/p&gt;</pre>
			</section>
			<section>
				<h2>Existing Work</h2>
				<ul>
					<li><a href="https://html.spec.whatwg.org/multipage/text-level-semantics.html#the-ruby-element">HTML Living Standard</a></li>
					<li><a href="https://schema.org/">Schema.org</a></li>
				</ul>
			</section>
			<section>
				<h2>Problems and Limitations</h2>
				<ul>
					<li>AT may process annotations as content</li>
					<li>AT "double reading" words instead of choosing either the content or the annotation</li>
					<li>Only offers for a few SSML expressions</li>
					<li>Difficult to reuse by reference</li>
				</ul>
			</section>
		</section>
		
		<section>
			<h1>User Scenarios</h1>
			<p>The purpose of developing user scenarios is to facilitate discussion and further requirements definition for pronunciation standards developed within the PTF prior to review of the APA. There are numerous interpretations of what form user scenarios adopt. Within the user experience research (UXR) body of practice, a user scenario is a written narrative related to the use of a service from the perspective of a user or user group. Importantly, the context of use is emphasized as is the desired outcome of use. There are potentially thousands of user scenarios for a technology such as TTS, however, the focus for the PTF is on the core scenarios that relate to the kinds of users who will engage with TTS.</p>
			<p>User scenarios, like Personas, represent a composite of real-world experiences. In the case of the PTF, the scenarios were derived from interviews of people who were end-consumers of TTS, as well as submitted narratives and industry examples from practitioners. There are several formats of scenarios. Several are general goal or task-oriented scenarios. Others elaborate on richer context, for example, educational assessment.</p>
			<p>The following user scenarios are organized on the three perspectives of TTS use derived from analysis of the qualitative data collected from the discovery work:</p>
			<ul>
				<li><strong>End-Consumers of TTS: </strong>Encompasses those with a visual disability or other need to have TTS operational when using assistive technologies (ATs).</li>
				<li><strong>Digital Content Managers: </strong>Addresses activities related to those responsible for producing content that needs to be accessible to ATs and W3C-WAI Guidelines.</li>
				<li><strong>Software Engineers: </strong>Includes developers and architects required to put TTS into an application or service.</li>
			</ul>
			<p class=issue>Need to add the other categories, or remove the list above and just rely on the ToC.</p>
		</section>
			
		<section>
			<h1>Augmentative and Alternative Communication (AAC)</h1>
			<section>
				<h2>Names</h2>
				<p>As an AAC User I want my name to be pronounced correctly and I want to pronounce others names correctly using my AAC device.</p>
				<section>
					<h3>Storing others' names</h3>
					<p>As an AAC user, I want to be able to input and store the correct pronunciation of others’ names, so I can address people respectfully and build meaningful relationships.</p>
					<p>For instance, when meeting someone named “Nguyễn,” the AAC user wants to ensure their device pronounces the name correctly, using IPA or SSML markup, to foster respectful communication and avoid embarrassment.</p>
				</section>
				<section>
					<h3>Pronouncing my name</h3>
					<p>As an AAC user, I want my name to be pronounced correctly by my device, so that I can confidently introduce myself in social, educational, and professional settings.</p>
					<p>For example, a user named “Siobhán” may find that default TTS engines mispronounce her name. She wants to input a phonetic or SSML-based pronunciation so that her name is spoken accurately every time.</p>
				</section>
			</section>
		</section>

		<section>
			<h2>End-Consumer of TTS</h2>
			<p>Ultimately, the quality and variation of TTS rendering by assistive technologies vary widely according to a user's context. The following user scenarios reinforce the necessity for accurate pronunciation from the perspective of those who consume digitally generated content.</p>
			<section>
				<h2>Traveller</h2>
				<p>As a traveler who uses assistive technology (AT) with TTS to help navigate through websites, I need to hear arrival and destination codes pronounced accurately so I can select the desired travel itinerary. For example, a user with a visual impairment attempts to book a flight to Ottawa, Canada and so goes to a travel website. The user already knows the airport code and enters "YOW". The site produces the result in a drop-down list as "Ottawa, CA" but the AT does not pronounce the text accurately to help the user make the correct association between their data entry and the list item.</p>
			</section>
			<section>
				<h2>Test Taker</h2>
				<p>As a test taker (tester) with a visual impairment who may use assistive technology to access the test content with speech software, screen reader or refreshable braille device, I want the content to be presented as intended, with accurate pronunciation and articulation, so that my assessment accurately reflects my knowledge of the content.</p>
			</section>
			<section>
				<h2>Student</h2>
				<p>As a student/learner with auditory and cognitive processing issues, it is difficult to distinguish sounds, inflections, and variations in pronunciation as rendered through synthetic voice, such as text-to-speech or screen reader technologies. Consistent and accurate pronunciation whether human-provided, external, or embedded is needed to support working executive processing, auditory processing and memory that facilitates comprehension in literacy and numeracy for learning and for assessments.</p>
			</section>
			<section>
				<h2>English learner</h2>
				<p>As an English Learner (EL) or a visually impaired early learner using speech synthesis for reading comprehension that includes decoding words from letters as part of the learning construct (intent of measurement), pronunciation accuracy is vital to successful comprehension, as it allows the learner to distinguish sounds at the sentence, word, syllable, and phoneme level.</p>
			</section>
		</section>

		<section>
			<h2>Digital Content Management for TTS</h2>
			<p>The advent of graphical user interfaces (GUIs) for the management and editing of text content has given rise to content creators not requiring technical expertise beyond the ability to operate a text editing application such as Microsoft Word. The following scenario summarizes the general use, accompanied by a hypothetical application.</p>
			<ul>
				<li>As a content creator, I want to create content that can readily be delivered through assistive technology, can convey the correct meaning, and ensure that screen readers render the right pronunciation based on the surrounding context.</li>
				<li>As a content producer for a global commercial site that is inclusive, I need to be able to provide accessible culture-specific content for different geographic regions.</li>
			</ul>

			<section>
				<h3>Educational Assessment</h3>
				<p>In the educational assessment field, providing accurate and concise pronunciation for students with auditory accommodations, such as text-to-speech (TTS) or students with screen readers, is vital for ensuring content validity and alignment with the intended construct, which objectively measures a test takers knowledge and skills. For test administrators/educators, pronunciations must be consistent across instruction and assessment in order to avoid test bias or impact effects for students. Some additional requirements for the test administrators, include, but are not limited to, such scenarios:</p>
				<section>
					<h3>Test Administrator&mdash;Read-aloud intonation, expression</h3>
					<p>As a test administrator, I want to ensure that students with the read-aloud accommodation, who are using assistive technology or speech synthesis as an alternative to a human reader, have the same speech quality (e.g., intonation, expression, pronunciation, and pace, etc.) as a spoken language.</p>
					<p class=issue>This may be simlar to the other Test Administrator case below?</p>
				</section>
				<section>
					<h3>Math educator</h3>
					<p>As a math educator, I want to ensure that speech accuracy with mathematical expressions, including numbers, fractions, and operations have accurate pronunciation for those who rely on TTS. Some mathematical expressions require special pronunciations to ensure accurate interpretation while maintaining test validity and construct. Specific examples include:</p>
					<section>
						<h4>Formulas</h4>
						<p>Mathematical formulas written in simple text with special formatting should convey the correct meaning of the expression to identify changes from normal text to super- or to sub-script text. For example, without the proper formatting, the equation:<code>a<sup>3</sup>-b<sup>3</sup>=(a-b)(a<sup>2</sup>+ab+b<sup>2</sup>)</code> may incorrectly render through some technologies and applications as a3-b3=(a-b)(a2+ab+b2).</p>
					</section>
					<section>
						<h4>Distinctions in writing</h4>
						<p>Distinctions made in writing are often not made explicit in speech; For example, “fx” may be interpreted as fx, f(x), fx, F X, F X. The distinction depends on the context; requiring the author to provide consistent and accurate semantic markup.</p>
					</section>
					<section>
						<h4>Greek letters</h4>
						<p>For math equations with Greek letters, it is important that the speech synthesizer be able to distinguish the phonetic differences between them, whether in the natural language or phonetic equivalents. For example, ε (epsilon) υ (upsilon) φ (phi) χ (chi) ξ(xi).</p>
					</section>
				</section>
				<section>
					<h3>Test Administrator&mdash;consistent pronunciation</h3>
					<p>As a test administrator/educator, pronunciations must be consistent across instruction and assessment, in order to avoid test bias and pronunciation effects on performance for students with disabilities (SWD) in comparison to students without disabilities (SWOD). Examples include:</p>
					<section>
						<h4>Spelling out rhyming words</h4>
						<p>If a test question is measuring rhyming of words or sounds of words, the speech synthesis should not read aloud the words, but rather spell out the words in the answer options.</p>
					</section>
					<section>
						<h4>Questions measuring spelling</h4>
						<p>If a test question is measuring spelling and the student needs to consider spelling correctness/incorrectness, the speech synthesis should not read aloud the misspelt words, especially for words, such as:</p>
						<ul>
							<li><strong>Heteronyms/homographs</strong>: same spelling, different pronunciation, different meanings, such as lead (to go in front of) or lead (a metal); wind (to follow a course that is not straight) or wind (a gust of air); bass (low, deep sound) or bass (a type of fish), etc.</li>
							<li><strong>Homophone</strong>: words that sound alike, such as, to/two/too; there/their/they're; pray/prey; etc.</li>
							<li><strong>Homonyms</strong>: multiple meaning words, such as scale (measure) or scale (climb, mount); fair (reasonable) or fair (carnival); suit (outfit) or suit (harmonize); etc.</li>
						</ul>
					</section>
				</section>
			</section>

			<section>
				<h3>Academic and Linguistic Practitioners </h3>
				<p>The extension of content management in TTS is one as a means of encoding and preserving spoken text for academic analyses; irrespective of discipline, subject domain, or research methodology.</p>
				<section>
					<h3>Linguist</h3>
					<p>A.	As a linguist, I want to represent all the pronunciation variations of a given word in any language, for future analyses.</p>
				</section>
				<section>
					<h3>Speech Language Pathologist, Speech Therapist</h3>
					<p>As a speech language pathologist or speech therapists, I want TTS functionality to include components of speech and language that include dialectal and individual differences in pronunciation; identify differences in intonation, syntax, and semantics, and; allow for enhanced comprehension, language processing and support phonological awareness.</p>
				</section>
			</section>
		</section>

		<section>
			<h2>Software Application Development</h2>
			<p>Technical standards for software development assist organizations and individuals to provide accessible experiences for users with disabilities. The final user scenarios in this document are considered from the perspective of those who design and develop software.</p>
			<p class=issue>Probably shouldn't use "final" here, as we mey re-order.</p>
			<section>
				<h3>Product owner</h3>
				<p>As a Product Owner for a web content management system (CMS), I want the next software product release to have the capability of pronouncing speech "just like Alexa can".</p>
			</section>
			<section>
				<h3>Client-side User Interface Developer</h3>
				<p>As a client-side user interface developer, I need a way to render text content, so it is spoken accurately with assistive technologies.</p>
			</section>
		</section>

		<div data-include="../common/acknowledgements.html" data-oninclude="fixIncludes" data-include-replace="true">Acknowledgements placeholder</div>

	</body>
</html>
